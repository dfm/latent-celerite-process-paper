% The layout of this document has been shamelessly stolen from the MIT licensed project:
% https://github.com/davidwhogg/GaussianProductRefactor
% by David W. Hogg, Adrian Price-Whelan, and Boris Leistedt

\documentclass[10pt]{article}

\usepackage{amsmath, bm, mathrsfs, amssymb,algorithm,algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks,
            colorlinks=true,
            linkcolor=NavyBlue,
            citecolor=darkgray,
            urlcolor=NavyBlue]{hyperref}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{doi}

\usepackage{natbib}
\bibliographystyle{dfm_abbrvnat}
\setcitestyle{round,citesep={,},aysep={}}

% text stuhh
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\equationname}{equation}
\newcommand{\notename}{Note}
\renewcommand{\tablename}{Table}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et~al.}}

% math stuhh
\newcommand{\hquad}{~~}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}\!}}
\newcommand{\inv}{^{-1}}
\newcommand{\scalar}[1]{#1}
\renewcommand{\vector}[1]{\boldsymbol{#1}}
\newcommand{\tensor}[1]{\mathbf{#1}}
\renewcommand{\matrix}[1]{\mathsf{#1}}
\newcommand{\normal}{\mathcal{N}\!\,}

% variables
\newcommand{\celerite}{\code{celerite}}

% Journal keys - kill me now
\newcommand{\aj}{Astron.\,J.}
\newcommand{\apj}{Astrophys.\,J.} % Astrophysical Journal
\newcommand{\apjs}{Astrophys.\,J.\,Supp.\,Ser.} % Astrophysical Journal Supplements

% page layout stuff
\setlength{\topmargin}{0.0in}
\setlength{\headheight}{0.0in}
\setlength{\headsep}{0.0in}
\setlength{\parindent}{\baselineskip}
\setlength{\textwidth}{4.3in}
\setlength{\textheight}{2\textwidth}
\raggedbottom\sloppy\sloppypar\frenchspacing

% this might be crazy, but here's my number
\setlength{\marginparsep}{0.15in}
\setlength{\marginparwidth}{2.7in}
\usepackage{marginfix} % necessary but possibly evil
\newcounter{marginnote}
\setcounter{marginnote}{0}
\renewcommand{\footnote}[1]{\refstepcounter{marginnote}\textsuperscript{\themarginnote}\marginpar{\color{darkgray}\raggedright\footnotesize\textsuperscript{\themarginnote}#1}}
\newcommand{\tfigurerule}{\rule{0pt}{1ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{0.25ex}}
\newcommand{\bfigurerule}{\rule{0pt}{0.25ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{1ex}}
\renewcommand{\caption}[1]{\parbox{\marginparwidth}{\footnotesize\refstepcounter{figure}\textbf{\figurename~\thefigure}: {#1}}}

% and make the left margin correct
\setlength{\oddsidemargin}{0.5\paperwidth}
\addtolength{\oddsidemargin}{-1.0in}
\addtolength{\oddsidemargin}{-0.5\textwidth}
\addtolength{\oddsidemargin}{-0.5\marginparwidth}
\addtolength{\oddsidemargin}{-0.5\marginparsep}

\begin{document}

\section*{Fast \& scalable computations with rank-$R$ semi-separable matrices\footnote{%
    The methods described here are available as part of the \href{https://github.com/exoplanet-dev/celerite2}{\code{exoplanet-dev/celerite2} project on GitHub} (CITE ZENODO) and the code used to make the figures for this paper are available as part of the \href{https://github.com/dfm/latent-celerite-process-paper}{\code{dfm/latent-celerite-process-paper} project on GitHub} (CITE ZENODO).}}

\noindent\textbf{Daniel Foreman-Mackey}\footnote{%
  The authors would like to thank the Astronomical Data Group at Flatiron for listening to every iteration of this project and for providing great feedback every step of the way.
  We would also like to thank
  Suzanne Aigrain (Oxford),
  Gregory Guida (Etsy),
  Christopher Stumm (Etsy),
  \emph{and others\ldots}
  for insightful conversations and other contributions.
}\\
{\footnotesize%
\textsl{Center for Computational Astrophysics, Flatiron Institute, New York, NY, USA}%
}

\medskip\noindent\textbf{Final author list and order TBD}\\
{\footnotesize%
\textsl{Affiliations}\\
\textsl{More affiliations}%
}

\paragraph{Abstract:}

% Probably better to re-write focusing more on the latent models instead of story time.

Matrices with semi-separable structure are at the core of some modern Gaussian Process algorithms and they allow efficient operations that are scalable in terms of computational cost and memory use.
In this note, we discuss efficient implementation of the standard matrix operations on semi-separable matrices and the forward- and reverse- mode automatic differentiation algorithms.
We also discuss the relationship with Kalman filtering methods for Gaussian Process regression.

\section{Introduction}

An $N$ by $N$, symmetric, rank-$R$ semi-separable matrix $K$ is defined as
\begin{eqnarray}
  K &=& \mathrm{diag}(A) + f(U,\, V) + f(U,\, V)\T
\end{eqnarray}
where $\mathrm{diag}(A)$ is a diagonal matrix, $f(U,\, V)$ is a lower triangular matrix, and both $U$ and $V$ are matrices with the shape $N \times R$.
The function $f(U,\, V)$ is often defined as
\begin{eqnarray}
  f(U,\, V) &=& \mathrm{tril}(U \, V\T)
\end{eqnarray}
where $\mathrm{tril}$ extracts the strict lower triangle of a square matrix, but it will be useful to allow more general definitions below.

Matrices with this structure are useful since many standard operations can be performed with $\mathcal{O}(N\,R)$ or $\mathcal{O}(N\,R^2)$ scaling for both computation and memory.

The key operations that we will discuss are:

\begin{enumerate}
  \item Matrix multiplication
  \item Cholesky factorization
  \item Triangular solves
  \item Diagonal of matrix-matrix multiplication
\end{enumerate}

We will also derive the forward- and reverse-mode differentiation rules for these operations and discuss their efficient implementation.

A paper. \citep{foreman-mackey2017,foreman-mackey2018,gordon2020}

\citep{loper2020,hu2020}

\section{Inclusive scan algorithm}

It turns out that we can re-write all of the standard operations on semi-separable matrices as inclusive scans with associate binary operators.
This is beneficial because there are well defined methods for parallelizing such algorithms and it will allow efficient parallel implementations on the GPU.

An ``inclusive scan'' or ``prefix sum'' operation for a sequence
\begin{eqnarray}
  (a_1,\, a_2,\, a_3,\, \ldots)
\end{eqnarray}
and binary operator $\otimes$ is defined as
\begin{eqnarray}
  (a_1,\, a_1 \otimes a_2,\, a_1 \otimes a_2 \otimes a_3,\, \ldots)\quad.
\end{eqnarray}
For example, if the operator is addition, then we would get the cumulative sum: each element becomes the sum of all earlier entries.

For semi-separable matrices, all of the operations can be written as inclusive scans like this.

\paragraph{Matrix multiply}
The algorithm for performing a lower triangular matrix multiply is listed in Figure~\ref{alg:tril-mat-mul}.
This can be rewritten as a scan operation where the state is
\begin{eqnarray}
  a_n &=& \left(\begin{array}{c}
    P_n \\
    P_n\,v_n\,{y_n}\T
  \end{array}\right)
\end{eqnarray}
and the binary operation $\otimes$ is defined as
\begin{eqnarray}
  a_n \otimes a_m &=& \left(\begin{array}{c}
    P_m\,P_n \\
    P_m\,P_n\,v_n\,{y_n}\T + P_m\,v_m\,{y_m}\T
  \end{array}\right) \quad.
\end{eqnarray}
We can demonstrate that this is an associative operation by demonstrating that
\begin{eqnarray}
  (a_n \otimes a_m) \otimes a_k &=& a_n \otimes (a_m \otimes a_k) \quad.
\end{eqnarray}


\begin{algorithm}
  \caption{Lower triangular matrix multiply\label{alg:tril-mat-mul}}
  \begin{algorithmic}[1]
    \Require{$P_n \in \mathbb{R}^{J \times J}$, $u_n \in \mathbb{R}^{J}$, $v_n \in \mathbb{R}^{J}$, and $y_n \in \mathbb{R}^{M}$}
    \Function{TrilMatMul}{$\{P_n,\, u_n,\, v_n,\, y_n\}_{n=1}^{N}$}
    \State{$F_1 \gets 0$} \Comment{$F_n \in \mathbb{R}^{J \times M}$}
    \State{$z_1 \gets 0$} \Comment{$z_n \in \mathbb{R}^{M}$}
    \For{$n \gets 2 \textrm{ to } N$}
    \State{$F_n \gets P_{n-1}\,\left(F_{n-1}+{v_{n-1}}\,{y_{n-1}}\T\right)$}
    \State{$z_n \gets {u_n}\T\,F_n$}
    \EndFor
    \State \Return{$Z$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Lower triangular matrix solve\label{alg:tril-mat-mul}}
  \begin{algorithmic}[1]
    \Require{$P_n \in \mathbb{R}^{J \times J}$, $u_n \in \mathbb{R}^{J}$, $v_n \in \mathbb{R}^{J}$, and $y_n \in \mathbb{R}^{M}$}
    \Function{TrilSolve}{$\{P_n,\, u_n,\, v_n,\, y_n\}_{n=1}^{N}$}
    \State{$F_1 \gets 0$} \Comment{$F_n \in \mathbb{R}^{J \times M}$}
    \State{$z_1 \gets y_n$} \Comment{$z_n \in \mathbb{R}^{M}$}
    \For{$n \gets 2 \textrm{ to } N$}
    \State{$F_n \gets P_{n-1}\,\left(F_{n-1}+{v_{n-1}}\,{z_{n-1}}\T\right)$}
    \State{$z_n \gets y_n - {u_n}\T\,F_n$}
    \EndFor
    \State \Return{$Z$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}


% \marginpar{\tfigurerule\\
%   \includegraphics[width=\marginparwidth, trim=0ex 0ex 0ex 0.5in, clip]{oned.pdf}
%   \caption{The one-dimensional case: If the prior pdf and the likelihood are both
%     Gaussian in a single parameter, their product (and hence the posterior pdf) is
%     also Gaussian, with a narrower width (smaller variance) than
%     either the prior pdf or the likelihood.\label{fig:oned}}\\
%   \bfigurerule}


% Render the references
\clearpage\raggedright
\bibliography{refs}

\end{document}
